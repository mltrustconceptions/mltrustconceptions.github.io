<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Trust Conceptions</title>
    <link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css">
    <link rel="stylesheet" href="custom.css">
</head>
<body>
  <header>
    <h1>Conceptions of Trust and Trustworthiness in ML</h1>
    <p>A Proposed NeurIPS 2024 Workshop</p>
    <p></p>
    <a class="button" href="index.html">Main</a>
    <a class="button" href="cfp.html">Call for Papers</a>
    <a class="button" href="people.html">People</a>
  </header>

  <main>
	  <h1>Organizers</h1>

		<center><img src="img/sarah_cen.jpg" width="300"></center>
		<p>
		<b>Sarah H. Cen</b> 
		is a final-year PhD student at MIT in the Electrical Engineering and Computer Science Department advised by Aleksander Mądry and Devavrat Shah. She will be joining Stanford Computer Science and Law School as a postdoc with Percy Liang and Daniel E. Ho Fall 2024, before joining Carnegie Mellon University as an Assistant Professor in 2026. Sarah utilizes methods from machine learning, statistics, causal inference, and game theory to study responsible computing and AI policy. Previously, she has written about social media, algorithmic fairness, algorithmic auditing, and AI supply chains. Sarah is deeply interested in understanding the meaning of “trust” and “trustworthiness” when referring to AI and ML, particularly as it is used by policymakers. She has previously written a piece that proposes a game-theoretic notion of trustworthiness in personalization settings that draws from insights in political science.
		</p>

		<p></p>


		<center><img src="img/chara_podimata.jpg" width="300"></center>
		<p>
		<b>Chara Podimata</b>
		 is an Assistant Professor of OR/Stat at MIT and a Lead Researcher at Archimedes/Athena RC. Broadly speaking, her work studies incentive-aware machine learning, social computing, online learning, and mechanism design. Recently, she started thinking about policy questions related to AI and recommendation systems. Before MIT, she was a FODSI postdoctoral fellow at UC Berkeley and she obtained her PhD in CS from Harvard, advised by Yiling Chen. She’s a recipient of an Amazon Research Award, a Microsoft Dissertation Grant, and a Siebel Scholarship.
		</p>

		<p></p>

		<center><img src="img/ben_laufer.jpg" width="300"></center>
		<p>
		<b>Benjamin Laufer</b> 
		is a PhD student at Cornell Tech, where he is advised by Jon Kleinberg and Helen Nissenbaum, and affiliated with the Digital Life Initiative and the AI Policy and Practice group. His research is on the social and ethical implications of algorithmic decision-making, with a recent focus on “general-purpose” machine learning models. Prior to joining Cornell, Ben worked as a data scientist and graduated from Princeton University with a degree in Operations Research and Financial Engineering. Outside of research, Ben is passionate about expanding educational opportunities – he has volunteered as a tutor in New Jersey prisons, mentored first- year PhD students, and currently serves as Cornell Tech’s faculty hiring representative.
		</p>

		<p></p>


		<center><img src="img/sarah_scheffler.jpg" width="300"></center>
		<p>
		<b>Sarah Scheffler</b> 
		is an assistant professor at Carnegie Mellon University, jointly appointed between “Engineering and Public Policy” and “Software and Societal Systems.”  Her research focuses on the intersection of cryptography, privacy, cybersecurity, and policy, with an emphasis on end-to-end encryption, content moderation, and privacy-preserving and verifiable computation.  Verification is a theme throughout her work, including her work on publicly verifiable private content moderation in encrypted settings, and on how courts should verify non-testimonial statements in 5th Amendment disputes.  Prior to joining CMU she was a postdoctoral researcher at MIT’s Internet Policy Research Institute, as well as at Princeton’s Center for Information Technology Policy, after receiving her Ph.D. from Boston University. 
		</p>

		<p></p>

		<center><img src="img/zoe_hitzig.jpg" width="300"></center>
		<p>
		<b>Zoë Hitzig</b> 
		is a Research Scientist at OpenAI and a Junior Fellow at
the Harvard Society of Fellows. Her work centers on privacy, equity and transparency in markets,
contracts and algorithms. She holds a PhD in economics from Harvard, where she was the recipient
of a Microsoft Research PhD Fellowship and a graduate fellowship from the Edmond J. Safra
Center for Ethics at Harvard.
		</p>

		<p></p>

		<center><img src="img/sayash_kapoor.jpg" width="300"></center>
		<p>
		<b>Sayash Kapoor</b> 
		is a computer science Ph.D. candidate at Princeton University's Center for Information Technology Policy. His research focuses on the societal impact of AI. He has previously worked on AI in industry and academia at Facebook, Columbia University, and EPFL Switzerland. He was also included in TIME's inaugural list of the 100 most influential people in AI.
He is am currently co-authoring a book on AI Snake Oil with Arvind Narayanan. The book looks critically at what AI can and cannot do.
		</p>

		<p></p>
  </main>

  <footer>
    <p>Conceptions of Trust and Trustworthiness in ML, NeurIPS 2024</p>
  </footer>
</body>
</html>


