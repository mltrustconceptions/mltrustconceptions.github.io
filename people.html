<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Trust Conceptions</title>
    <link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css">
    <link rel="stylesheet" href="custom.css">
</head>
<body>
  <header>
    <h1>Conceptions of Trust and Trustworthiness in ML</h1>
    <p>A Proposed NeurIPS 2024 Workshop</p>
    <p></p>
    <a class="button" href="index.html">Main</a>
    <a class="button" href="cfp.html">Call for Papers</a>
    <a class="button" href="people.html">People</a>
  </header>

  <main>
	  <h1>Organizers</h1>


		<div class="bio-container">
		<div class="bio-picture"><img src="https://news.mit.edu/sites/default/files/styles/news_article__image_gallery/public/images/202205/Sarah_Cen_LIDS_MIT.jpg"></div>
  		<div class="bio-text">
			<p>
		<b>Sarah H. Cen</b> 
		is a final-year PhD student at MIT in the Electrical Engineering and Computer Science Department advised by Aleksander Mądry and Devavrat Shah. She will be joining Stanford Computer Science and Law School as a postdoc with Percy Liang and Daniel E. Ho Fall 2024, before joining Carnegie Mellon University as an Assistant Professor in 2026. Sarah utilizes methods from machine learning, statistics, causal inference, and game theory to study responsible computing and AI policy. Previously, she has written about social media, algorithmic fairness, algorithmic auditing, and AI supply chains. Sarah is deeply interested in understanding the meaning of “trust” and “trustworthiness” when referring to AI and ML, particularly as it is used by policymakers. She has previously written a piece that proposes a game-theoretic notion of trustworthiness in personalization settings that draws from insights in political science.
			</p>
  		</div>
		</div>


		<div class="bio-container">
		<div class="bio-picture"><img src="https://www.charapodimata.com/files/charapod-headshot-new.jpg"></div>
  		<div class="bio-text">
		<p>
		<b>Chara Podimata</b>
		 is an Assistant Professor of OR/Stat at MIT and a Lead Researcher at Archimedes/Athena RC. Broadly speaking, her work studies incentive-aware machine learning, social computing, online learning, and mechanism design. Recently, she started thinking about policy questions related to AI and recommendation systems. Before MIT, she was a FODSI postdoctoral fellow at UC Berkeley and she obtained her PhD in CS from Harvard, advised by Yiling Chen. She’s a recipient of an Amazon Research Award, a Microsoft Dissertation Grant, and a Siebel Scholarship. Outside of research, she loves running, crocheting, and adventuring with her dog, Terra.
		</p>
  		</div>
		</div>

		<div class="bio-container">
		<div class="bio-picture"><img src="https://infosci.cornell.edu/sites/default/files/Laufer%2C%20Ben_0.png"></div>
  		<div class="bio-text">
		<p>
		<b>Benjamin Laufer</b> 
		is a PhD student at Cornell Tech, where he is advised by Jon Kleinberg and Helen Nissenbaum, and affiliated with the Digital Life Initiative and the AI Policy and Practice group. His research is on the social and ethical implications of algorithmic decision-making, with a recent focus on “general-purpose” machine learning models. Prior to joining Cornell, Ben worked as a data scientist and graduated from Princeton University with a degree in Operations Research and Financial Engineering. Outside of research, Ben is passionate about expanding educational opportunities – he has volunteered as a tutor in New Jersey prisons, mentored first- year PhD students, and currently serves as Cornell Tech’s faculty hiring representative.
		</p>
  		</div>
		</div>



		<div class="bio-container">
		<div class="bio-picture"><img src="https://sarahscheffler.net/sarahscheffler.jpg"></div>
  		<div class="bio-text">
		<p>
		<b>Sarah Scheffler</b> 
		is an assistant professor at Carnegie Mellon University, jointly appointed between “Engineering and Public Policy” and “Software and Societal Systems.”  Her research focuses on the intersection of cryptography, privacy, cybersecurity, and policy, with an emphasis on end-to-end encryption, content moderation, and privacy-preserving and verifiable computation.  Verification is a theme throughout her work, including her work on publicly verifiable private content moderation in encrypted settings, and on how courts should verify non-testimonial statements in 5th Amendment disputes.  Prior to joining CMU she was a postdoctoral researcher at MIT’s Internet Policy Research Institute, as well as at Princeton’s Center for Information Technology Policy, after receiving her Ph.D. from Boston University. 
		</p>
  		</div>
		</div>


		<div class="bio-container">
		<div class="bio-picture"><img src="https://images.squarespace-cdn.com/content/v1/5817be826b8f5bc983f829e2/a9aaebe3-e450-48ec-a5ac-5ae06d799e48/photo.jpeg?format=2500w"></div>
  		<div class="bio-text">
		<p>
		<b>Zoë Hitzig</b> 
is a Junior Fellow at the Harvard Society of Fellows.
She received her Ph.D. in economics from Harvard in 2023.
Her work centers on privacy and transparency in markets, contracts and other forms of communication.
		</p>
  		</div>
		</div>

		<div class="bio-container">
		<div class="bio-picture"><img src="https://mraghavan.github.io/images/profile.png"></div>
  		<div class="bio-text">
		<p>
		<b>Manish Raghavan</b> 
		 is the Drew Houston (2005) Career Development Professor at the MIT Sloan School of Management (in the Information Technology group) and department of Electrical Engineering and Computer Science.
His primary interests lie in the application of computational techniques to domains of social concern, including online platforms, algorithmic fairness, and behavioral economics, with a particular focus on the use of algorithmic tools in the hiring pipeline. He is also a member of Cornell's Artificial Intelligence, Policy, and Practice initiative.
		</p>
  		</div>
		</div>

		<div class="bio-container">
		<div class="bio-picture"><img src="https://www.cs.princeton.edu/~sayashk/sayash.png"></div>
  		<div class="bio-text">
		<p>
		<b>Sayash Kapoor</b> 
		is a computer science Ph.D. candidate at Princeton University's Center for Information Technology Policy. His research focuses on the societal impact of AI. He has previously worked on AI in industry and academia at Facebook, Columbia University, and EPFL Switzerland. He was also included in TIME's inaugural list of the 100 most influential people in AI.
He is am currently co-authoring a book on AI Snake Oil with Arvind Narayanan. The book looks critically at what AI can and cannot do.
		</p>
  		</div>
		</div>
  </main>

  <footer>
    <p>Conceptions of Trust and Trustworthiness in ML, NeurIPS 2024</p>
  </footer>
</body>
</html>


