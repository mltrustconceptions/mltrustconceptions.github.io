<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Trust Conceptions</title>
    <link rel="stylesheet" href="https://cdn.simplecss.org/simple.min.css">
    <link rel="stylesheet" href="custom.css">
</head>
<body>
  <header>
    <h1>Conceptions of Trust and Trustworthiness in ML</h1>
    <p>A Proposed NeurIPS 2024 Workshop</p>
    <p></p>
    <a class="button" href="index.html">Main</a>
    <a class="button" href="cfp.html">Call for Papers</a>
    <a class="button" href="people.html">People</a>
  </header>

  <main>
	  <p>
	  Machine learning (ML) systems are rapidly being introduced across a variety of sectors, impacting how we interact with one another and access information. 
	  As a result, many are calling for “trustworthy” AI systems – from President Biden’s 
	  <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">executive order</a>
	  to the 
	  <a href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">EU's AI Act</a>
	  to the 
	  <a href="https://aiforgood.itu.int/">AI For Good</a> 
	  initiative.  
	  These calls emerge from a host of studies and experiences that have highlighted important limitations in the reliable use of machine learning at scale: 
	  error, hallucinations, bias, dangerous safety concerns, and harms emerging from the systems themselves. 
	  Researchers, policy-makers and practitioners are all looking to understand, create, and oblige trust and trustworthiness in ML.
	  </p>

	  <p>
	  However, both the calls and the responses have made it apparent that “trust” and “trustworthiness” mean different things to different people, groups, and fields of study.  
	  What does it mean for a person to “trust” a computationally-driven machine? Some conceptions of “trust” are a technical definition a ML model must meet.  
	  Some notions of “trust” reflect the processes through which the model was generated, or through which it is used, more than any property of the model itself.  
	  Still other perceptions are based on individuals’ and groups’ beliefs about the model and its developers, data contributors, deployers, users, or subjects.  
	  Among this wide variety of conceptions of “trust” and “tustworthiness” surrounding machine learning, a solid foundation is needed to connect existing scholarship and develop a future program of research. 
	  </p>

	  <p>
	  This workshop brings together experts across a range of backgrounds, methods and disciplines, featuring researchers from academia and industry, theory and practice, 
	  who all bring different societal and individual lenses to the topic of trust.  
	  This workshop’s goal is for participants to learn, compare, and synthesize different definitions of trust and how they can be operationalized in machine learning.  
	  </p>

	  <p>
	  We encourage submissions related to the following topics:
	  </p>
	  <ul>
		<li>Algorithmic auditing</li>
		<li>Economic approaches (e.g., to consumer confidence and firm reputations)</li>
		<li>Explanations and explainability</li>
		<li>Participatory machine learning </li>
		<li>Philosophical/ethical foundations</li>
		<li>Reliable predictions and reporting them responsibly</li>
		<li>Reproducibility, including in statistical estimates of treatment effects</li>
		<li>Social choice and other political-theoretic approaches to designing AI systems.</li>
	  </ul>

	  <p>
	  For those interested, we will additionally use the workshop convening as a starting point to develop a paper that makes explicit the theoretical foundations and future directions for this budding research program. 
	  There will be an optional working session at the end of the day to discuss next steps in this direction.
	  </p>


  </main>

  <footer>
    <p>Conceptions of Trust and Trustworthiness in ML, NeurIPS 2024</p>
  </footer>
</body>
</html>

